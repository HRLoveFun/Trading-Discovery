{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from scipy.optimize import minimize_scalar\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "\n",
    "# import os\n",
    "# from fredapi import Fred\n",
    "\n",
    "from marketobserve import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Return Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "start = dt.date(1900, 1, 1)\n",
    "end = dt.date(2026, 1, 1)\n",
    "ticker = \"^HSI\"\n",
    "data = yf_download(ticker, start, end)\n",
    "# data.index = data.index.strftime('%Y-%m')\n",
    "# data.to_excel(f\"{ticker}.xlsx\")\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_sources(df, periods, all_period_start, frequency):\n",
    "    \"\"\"\n",
    "    创建不同时间周期的数据来源\n",
    "    \"\"\"\n",
    "    current_date = pd.Timestamp.now()\n",
    "\n",
    "    # 根据 frequency 筛选数据到本周/本月/本季度的第一天\n",
    "    if frequency == 'ME':\n",
    "        end_date = current_date.replace(day=1)\n",
    "    elif frequency == 'W':\n",
    "        end_date = current_date - pd.DateOffset(days=current_date.weekday())\n",
    "    elif frequency == 'QE':\n",
    "        end_date = current_date - pd.tseries.offsets.QuarterBegin()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid frequency value. Allowed values are 'ME', 'W', 'QE'.\")\n",
    "\n",
    "    df = df[df.index < end_date]\n",
    "    last_date = df.index[-1]\n",
    "\n",
    "    if all_period_start is None:\n",
    "        all_period_start = \"2010-01-01\"\n",
    "\n",
    "    data_sources = {}\n",
    "    for period in periods:\n",
    "        if isinstance(period, int):\n",
    "            if frequency in ['ME', 'W']:\n",
    "                start_date = last_date - pd.DateOffset(months=period - 1)\n",
    "            elif frequency == 'QE':\n",
    "                start_date = last_date - pd.DateOffset(quarters=period - 1)\n",
    "            col_name = f\"{start_date.strftime('%y%b')}-{last_date.strftime('%y%b')}\"\n",
    "            data_sources[col_name] = df.loc[df.index >= start_date]\n",
    "        elif period == \"ALL\":\n",
    "            col_name = f\"{pd.to_datetime(all_period_start).strftime('%y%b')}-{last_date.strftime('%y%b')}\"\n",
    "            data_sources[col_name] = df.loc[df.index >= all_period_start]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid period value\")\n",
    "\n",
    "    return data_sources\n",
    "\n",
    "\n",
    "def refrequency(df, frequency: str):\n",
    "    \"\"\"\n",
    "    对数据进行重采样\n",
    "    \"\"\"\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"DataFrame index must be a DatetimeIndex\")\n",
    "    if not {'Open', 'High', 'Low', 'Close'}.issubset(df.columns):\n",
    "        raise ValueError(\"DataFrame must contain OHLC columns\")\n",
    "\n",
    "    try:\n",
    "        refrequency_df = df.resample(frequency).agg({\n",
    "            'Open': 'first',\n",
    "            'High': 'max',\n",
    "            'Low': 'min',\n",
    "            'Close': 'last',\n",
    "            'Adj Close': 'last',\n",
    "            'Volume': 'sum'\n",
    "        }).dropna()\n",
    "\n",
    "    except KeyError as e:\n",
    "        import logging\n",
    "        logging.error(f\"Missing column {e} in DataFrame\")\n",
    "        raise ValueError(f\"Error processing data: Missing column {e}\")\n",
    "    except Exception as e:\n",
    "        import logging\n",
    "        logging.error(f\"Unexpected error: {str(e)}\")\n",
    "        raise ValueError(f\"Error processing data: {str(e)}\")\n",
    "\n",
    "    return refrequency_df\n",
    "\n",
    "\n",
    "def oscillation(df):\n",
    "    \"\"\"\n",
    "    计算震荡指标\n",
    "    \"\"\"\n",
    "    data = df[['Open', 'High', 'Low', 'Close']].copy()\n",
    "    data['LastClose'] = data[\"Close\"].shift(1)\n",
    "    data[\"Oscillation\"] = data[\"High\"] - data[\"Low\"]\n",
    "    data[\"OscillationPct\"] = (data[\"Oscillation\"] / data['LastClose'])\n",
    "    data = data.dropna()\n",
    "    return data\n",
    "\n",
    "\n",
    "def tail_stats(df, feature, frequency, periods: list = [12, 36, 60, \"ALL\"], all_period_start: str = None, interpolation: str = \"linear\"):\n",
    "    \"\"\"\n",
    "    计算不同时间周期的统计指标\n",
    "    \"\"\"\n",
    "    if not isinstance(periods, list):\n",
    "        raise TypeError(\"periods must be a list\")\n",
    "    if not all(isinstance(p, (int, str)) for p in periods):\n",
    "        raise ValueError(\"periods must contain integers or strings\")\n",
    "\n",
    "    data_sources = create_data_sources(df, periods, all_period_start, frequency)\n",
    "\n",
    "    stats_index = pd.Index([\"mean\", \"std\", \"skew\", \"kurt\", \"max\", \"99th\", \"95th\", \"90th\"])\n",
    "    stats_df = pd.DataFrame(index=stats_index)\n",
    "\n",
    "    for period_name, data in data_sources.items():\n",
    "        stats_df[period_name] = [\n",
    "            data[feature].mean(),\n",
    "            data[feature].std(),\n",
    "            data[feature].skew(),\n",
    "            data[feature].kurtosis(),\n",
    "            data[feature].max(),\n",
    "            data[feature].quantile(0.99, interpolation=interpolation),\n",
    "            data[feature].quantile(0.95, interpolation=interpolation),\n",
    "            data[feature].quantile(0.90, interpolation=interpolation)\n",
    "        ]\n",
    "\n",
    "    return stats_df\n",
    "\n",
    "\n",
    "def tail_plot(df, feature, frequency, periods: list = [12, 36, 60, \"ALL\"], all_period_start: str = None, interpolation: str = \"linear\"):\n",
    "    \"\"\"\n",
    "    绘制不同时间周期的特征值分布\n",
    "    \"\"\"\n",
    "    data_sources = create_data_sources(df, periods, all_period_start, frequency)\n",
    "\n",
    "    if frequency == \"ME\":\n",
    "        bin_range = list(np.arange(0, 0.35, 0.05))\n",
    "    elif frequency == \"W\":\n",
    "        bin_range = list(np.arange(0, 0.18, 0.03))\n",
    "\n",
    "    for period_name, data in data_sources.items():\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.set_style(\"darkgrid\")\n",
    "        n, bins, patches = plt.hist(data[feature], bins=bin_range, alpha=0.5, color='skyblue', density=True, cumulative=True)\n",
    "\n",
    "        n_diff = np.insert(np.diff(n), 0, n[0])\n",
    "        for rect, h_diff, h in zip(patches, n_diff, n):\n",
    "            height = rect.get_height()\n",
    "            plt.text(rect.get_x() + rect.get_width() / 2, height, f'{h_diff:.0%}/{h:.0%}', ha='center', va='bottom', size=12)\n",
    "\n",
    "        percentiles = [data[feature].quantile(p, interpolation=interpolation) for p in [0.90, 0.95, 0.99]]\n",
    "        for p, val in zip([90, 95, 99], percentiles):\n",
    "            plt.axvline(val, color='red', linestyle=':', alpha=0.3, label=f'{p}th: {val:.1%}')\n",
    "\n",
    "        last_three = data[feature].iloc[-3:]\n",
    "        last_three_dates = last_three.index.strftime('%b%d')\n",
    "        for val, date, grayscale in zip(last_three, last_three_dates, np.arange(0.7, 0, -0.3)):\n",
    "            plt.scatter(val, 0, color=str(grayscale), s=100, zorder=5, label=f'{date}: {val:.1%}')\n",
    "\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.title(f\"Distribution of {feature} - {period_name}\")\n",
    "        plt.xlabel(f\"{feature} (%)\")\n",
    "        plt.ylabel(\"Density\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def calculate_projections(data, feature, percentile, interpolation, bias_weight):\n",
    "    data[\"ProjectHigh\"] = data[\"LastClose\"] + data[\"LastClose\"] * data[feature].quantile(percentile, interpolation=interpolation) / 100 * bias_weight\n",
    "    data[\"ProjectLow\"] = data[\"LastClose\"] - data[\"LastClose\"] * data[feature].quantile(percentile, interpolation=interpolation) / 100 * (1 - bias_weight)\n",
    "    data[\"ActualClosingStatus\"] = np.where(data[\"Close\"] > data[\"ProjectHigh\"], 1,\n",
    "                                           np.where(data[\"Close\"] < data[\"ProjectLow\"], -1, 0))\n",
    "    realized_bias = ((data[\"ActualClosingStatus\"] == 1).sum() - ((data[\"ActualClosingStatus\"] == -1).sum())) / len(data)\n",
    "\n",
    "    return realized_bias\n",
    "\n",
    "\n",
    "def volatility_projection(df, feature, frequency: str = 'ME', percentile: float = 0.90, prefer_bias: float = None, periods: list = [12, 36, 60, \"ALL\"], all_period_start: str = None, interpolation: str = \"linear\"):\n",
    "    \"\"\"\n",
    "    计算不同时间周期的波动率预测\n",
    "    \"\"\"\n",
    "    if not isinstance(periods, list):\n",
    "        raise TypeError(\"periods must be a list\")\n",
    "    if not all(isinstance(p, (int, str)) for p in periods):\n",
    "        raise ValueError(\"periods must contain integers or strings\")\n",
    "\n",
    "    if feature == \"OscillationPct\":\n",
    "        refrequency_data = refrequency(df, frequency=frequency)\n",
    "        refrequency_feature = oscillation(refrequency_data)\n",
    "\n",
    "        data_sources = create_data_sources(refrequency_feature, periods, all_period_start, frequency)\n",
    "\n",
    "        volatility_projection_index = pd.Index(\n",
    "            [\n",
    "                f\"Last: {refrequency_feature.index[-2].strftime('%y%b%d')}\",\n",
    "                f\"{percentile}th {feature}\",\n",
    "                \"RealizedBias%\",\n",
    "                \"ProjectedHighWeight%\",\n",
    "                \"ProjHigh\",\n",
    "                \"ProjLow\",\n",
    "                f\"Today: {df.index[-1].strftime('%y%b%d')}\"\n",
    "            ]\n",
    "        )\n",
    "        volatility_projection_df = pd.DataFrame(index=volatility_projection_index)\n",
    "\n",
    "        for period_name, data in data_sources.items():\n",
    "            period_end_close = data[\"Close\"].iloc[-1]\n",
    "            assumed_volatility = data[feature].quantile(percentile, interpolation=interpolation)\n",
    "\n",
    "            if prefer_bias is not None:\n",
    "                # 寻找最佳 bias_weight 的逻辑\n",
    "                proj_high_weights = np.linspace(0.1, 0.9, 90)  # 在 0 到 1 之间生成 100 个等间距的 bias_weight 值\n",
    "                min_error = float('inf')\n",
    "                best_proj_high_weight = 0\n",
    "                for proj_high_weight in proj_high_weights:\n",
    "                    realized_bias = calculate_projections(data.copy(), feature, percentile, interpolation, proj_high_weight)\n",
    "                    error = abs(realized_bias - prefer_bias)\n",
    "                    if error < min_error:\n",
    "                        min_error = error\n",
    "                        best_proj_high_weight = proj_high_weight\n",
    "                proj_high_weight = best_proj_high_weight\n",
    "\n",
    "            else:\n",
    "                proj_high_weight = 0.5\n",
    "\n",
    "            realized_bias = calculate_projections(data, feature, percentile, interpolation, proj_high_weight)\n",
    "\n",
    "            proj_high = period_end_close + period_end_close * assumed_volatility * proj_high_weight\n",
    "            proj_low = period_end_close - period_end_close * assumed_volatility * (1 - proj_high_weight)\n",
    "\n",
    "            last_close = df[\"Close\"].iloc[-1]\n",
    "\n",
    "            volatility_projection_df[period_name] = [\n",
    "                period_end_close,\n",
    "                assumed_volatility,\n",
    "                realized_bias * 100,\n",
    "                proj_high_weight * 100,\n",
    "                proj_high,\n",
    "                proj_low,\n",
    "                last_close\n",
    "            ]\n",
    "        return volatility_projection_df\n",
    "    else:\n",
    "        raise ValueError(\"Invalid feature value\")\n",
    "\n",
    "\n",
    "def days_of_frequency(frequency):\n",
    "    if frequency == \"W\":\n",
    "        days = 5\n",
    "    elif frequency == \"ME\":\n",
    "        days = 20\n",
    "    elif frequency == \"QE\":\n",
    "        days = 60\n",
    "    else:\n",
    "        raise ValueError(\"Invalid frequency, input one of ['W', 'ME', 'QE']\")\n",
    "\n",
    "    return days\n",
    "\n",
    "\n",
    "def tail_table(df, feature, frequency, periods: list = [12, 36, 60, \"ALL\"], all_period_start: str = None, interpolation: str = \"linear\"):\n",
    "    \"\"\"\n",
    "    **To Modify Ouput Table Information** 按每个时间段计算表格，输出每个时间段及其对应的表格，最后将结果存储在字典中返回\n",
    "    \"\"\"\n",
    "    data_sources = create_data_sources(df, periods, all_period_start, frequency)\n",
    "\n",
    "    if frequency == \"ME\":\n",
    "        bin_range = list(np.arange(0, 0.35, 0.05))\n",
    "    elif frequency == \"W\":\n",
    "        bin_range = list(np.arange(0, 0.18, 0.03))\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported frequency: {frequency}. Supported frequencies are 'ME' and 'W'.\")\n",
    "\n",
    "    result_df = pd.DataFrame()\n",
    "    for period_name, data in data_sources.items():\n",
    "        # 计算直方图的累积密度\n",
    "        n, bins = np.histogram(data[feature], bins=bin_range, density=True)\n",
    "        cumulative_n = np.cumsum(n * np.diff(bins))\n",
    "        n_diff = np.insert(np.diff(cumulative_n), 0, cumulative_n[0])\n",
    "\n",
    "        percentiles = [0.90, 0.95, 0.99]\n",
    "\n",
    "        # 计算百分位数\n",
    "        percentile_values = [data[feature].quantile(p, interpolation=interpolation) for p in percentiles]\n",
    "\n",
    "        # 获取最后三个数据点\n",
    "        last_three_values = data[feature].iloc[-3:]\n",
    "        last_three_dates = last_three_values.index.strftime('%b%d')\n",
    "        bin_intervals = [(bins[i], bins[i + 1]) for i in range(len(bins) - 1)]\n",
    "\n",
    "        bin_info = {\n",
    "            \"Period\": period_name,\n",
    "        }\n",
    "\n",
    "        for _ in range(len(bin_intervals)):\n",
    "            bin_info[f\"{bin_intervals[_]}\"] = f'{n_diff[_]:.0%}'\n",
    "\n",
    "        for _ in range(len(percentiles)):\n",
    "            bin_info[f\"{percentiles[_]}th\"] = f'{percentile_values[_]:.1%}'\n",
    "\n",
    "        for _ in range(len(last_three_dates)):\n",
    "            bin_info[f\"{last_three_dates[_]}\"] = f'{last_three_values.iloc[_]:.1%}'\n",
    "\n",
    "        table = pd.DataFrame([bin_info])\n",
    "        result_df = pd.concat([result_df, table], ignore_index=True)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def period_gap_stats(df, feature, frequency, periods: list = [12, 36, 60, \"ALL\"], all_period_start: str = None, interpolation: str = \"linear\"):\n",
    "    \"\"\"\n",
    "    计算不同时间周期的频率缺口统计\n",
    "    Given df, feature, and frequency,\n",
    "        for each period in frequency,\n",
    "            compute the gap_return = percentage change of first date open over last period close\n",
    "            compute statistics of gap_return\n",
    "            compute frequency_return = percentage change of last date close over last period close\n",
    "            set days_of_period = len(df[rows only in the period])\n",
    "            compare distribution of (gap_return+1)**days_of_period-1 with frequency_return distribution\n",
    "        return distribution table of gap_return \n",
    "    \"\"\"\n",
    "    data_sources = create_data_sources(df, periods, all_period_start, frequency)\n",
    "\n",
    "    stats_index = pd.Index([\"mean\", \"std\", \"skew\", \"kurt\", \"max\", \"99th\", \"95th\", \"90th\", \"10th\", \"05th\", \"01st\", \"min\", \"p-value\"])\n",
    "    gap_return_stats_df = pd.DataFrame(index=stats_index)\n",
    "\n",
    "    if feature == \"PeriodGap\":\n",
    "        for period_name, data in data_sources.items():\n",
    "            if len(data) > 0:\n",
    "                # 计算 gap_return\n",
    "                gap_return = (data[\"Open\"] / data[\"LastClose\"] - 1)\n",
    "                period_return = (data[\"Close\"] / data[\"LastClose\"] - 1)\n",
    "\n",
    "                # 计算 days_of_period\n",
    "                days_of_period = days_of_frequency(frequency)\n",
    "\n",
    "                # 计算 (gap_return+1)**days_of_period-1\n",
    "                compounded_gap_return = (1 + gap_return) ** days_of_period - 1\n",
    "                # 计算 period_return 和 compounded_gap_return 相似程度的统计检验\n",
    "                _, p_value = ks_2samp(compounded_gap_return, period_return)\n",
    "\n",
    "                # 计算 gap_return 的统计信息\n",
    "                gap_return_stats_df[period_name] = [\n",
    "                    gap_return.mean(),\n",
    "                    gap_return.std(),\n",
    "                    gap_return.skew(),\n",
    "                    gap_return.kurtosis(),\n",
    "                    gap_return.max(),\n",
    "                    gap_return.quantile(0.99, interpolation=interpolation),\n",
    "                    gap_return.quantile(0.95, interpolation=interpolation),\n",
    "                    gap_return.quantile(0.90, interpolation=interpolation),\n",
    "                    gap_return.quantile(0.10, interpolation=interpolation),\n",
    "                    gap_return.quantile(0.05, interpolation=interpolation),\n",
    "                    gap_return.quantile(0.01, interpolation=interpolation),\n",
    "                    gap_return.min(),\n",
    "                    p_value\n",
    "                ]\n",
    "\n",
    "    return gap_return_stats_df\n",
    "\n",
    "\n",
    "def option_matrix(ticker, option_holding):\n",
    "    \"\"\"\n",
    "    option_holding: dataframe. columns: type: values of [LC,SC,LP,SP]; strike: integer, premium: float\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the last price of ticker\n",
    "    close = yf.download(ticker, start=dt.datetime.now().date())[\"Close\"].iloc[-1,-1]\n",
    "\n",
    "    # Create default option_matrix dataframe, columns = ['Price', 'SC', 'SP', 'LC', 'LS'], index = range of close*0.8, close*1.20\n",
    "    price_range = np.arange(0.9, 1.11, 0.01)\n",
    "    option_matrix_df = pd.DataFrame(index=price_range)\n",
    "    option_matrix_df['price'] = (close*price_range).astype(int)\n",
    "    option_matrix_df['SC'] = 0.0\n",
    "    option_matrix_df['SP'] = 0.0\n",
    "    option_matrix_df['LC'] = 0.0\n",
    "    option_matrix_df['LP'] = 0.0\n",
    "\n",
    "    for _, row in option_holding.iterrows():\n",
    "        option_type = row[\"option_type\"]\n",
    "        strike = row[\"strike\"]\n",
    "        premium = row[\"premium\"]\n",
    "\n",
    "        print(f\"Option type: {option_type}, Strike: {strike}, Premium: {premium}\")\n",
    "\n",
    "        if option_type == 'SC':\n",
    "            option_matrix_df.loc[option_matrix_df.price < strike, 'SC'] = premium\n",
    "            option_matrix_df.loc[option_matrix_df.price >= strike, 'SC'] =  premium + (strike - option_matrix_df.loc[option_matrix_df.price >= strike,'price'])\n",
    "        elif option_type == 'SP':\n",
    "            option_matrix_df.loc[option_matrix_df.price > strike, 'SP'] = premium\n",
    "            option_matrix_df.loc[option_matrix_df.price <= strike, 'SP'] = premium - (strike - option_matrix_df.loc[option_matrix_df.price <= strike,'price'])\n",
    "        elif option_type == 'LC':\n",
    "            option_matrix_df.loc[option_matrix_df.price > strike, 'LC'] = option_matrix_df.loc[option_matrix_df.price > strike,'price'] - strike - premium\n",
    "            option_matrix_df.loc[option_matrix_df.price <= strike, 'LC'] = - premium\n",
    "        elif option_type == 'LP':\n",
    "            option_matrix_df.loc[option_matrix_df.price > strike, 'LP'] =  - premium\n",
    "            option_matrix_df.loc[option_matrix_df.price <= strike, 'LP'] = - option_matrix_df.loc[option_matrix_df.price <= strike,'price'] + strike - premium\n",
    "        else:\n",
    "            raise ValueError(\"Invalid option type\")\n",
    "\n",
    "    return option_matrix_df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency = 'ME'\n",
    "# monthly_data = refrequency(data, frequency=frequency)\n",
    "# monthly_oscill = oscillation(monthly_data)\n",
    "# monthly_tail_stats_result =tail_stats(monthly_oscill,\"OscillationPct\",frequency=frequency)\n",
    "# print(monthly_tail_stats_result,\"\\n\")\n",
    "\n",
    "# round_digit = 1\n",
    "# tail_plot(monthly_oscill,\"OscillationPct\",frequency=frequency)\n",
    "# volatility_proj_pbn = volatility_projection(data,\"OscillationPct\",frequency=frequency,prefer_bias=None).round(round_digit)\n",
    "# print(volatility_proj_pbn,\"\\n\")\n",
    "# volatility_proj_pb0 = volatility_projection(data,\"OscillationPct\",frequency=frequency,prefer_bias=0).round(round_digit)\n",
    "# print(volatility_proj_pb0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency = 'W'\n",
    "# refrequency_data = refrequency(data, frequency=frequency)\n",
    "# oscill = oscillation(refrequency_data)\n",
    "# tail_stats_result =tail_stats(oscill,\"OscillationPct\",frequency=frequency)\n",
    "# print(tail_stats_result.apply(lambda row: row.apply(lambda x: '{:.2%}'.format(x) if isinstance(x, (int, float)) and row.name not in [\"skew\", \"kurt\", \"p-value\"] else '{:.2f}'.format(x)),axis=1),\"\\n\")\n",
    "\n",
    "# round_digit = 2\n",
    "# # tail_plot(oscill,\"OscillationPct\",frequency=frequency)\n",
    "# tail_hist = tail_table(oscill,\"OscillationPct\",frequency=frequency)\n",
    "# print(tail_hist,\"\\n\")\n",
    "# volatility_proj_pbn = volatility_projection(data,\"OscillationPct\",frequency=frequency,prefer_bias=None).round(round_digit)\n",
    "# print(volatility_proj_pbn,\"\\n\")\n",
    "# volatility_proj_pb0 = volatility_projection(data,\"OscillationPct\",frequency=frequency,prefer_bias=0).round(round_digit)\n",
    "# print(volatility_proj_pb0)\n",
    "# gap_stats_result = period_gap_stats(oscill,\"PeriodGap\",frequency=frequency)\n",
    "# (gap_stats_result).apply(lambda row: row.apply(lambda x: '{:.2%}'.format(x) if isinstance(x, (int, float)) and row.name not in [\"skew\", \"kurt\", \"p-value\"] else '{:.2f}'.format(x)),axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "option_holding = pd.DataFrame(\n",
    "    {\n",
    "        \"option_type\":[\"SC\",\"SP\",\"LC\",\"LP\"],\n",
    "        \"strike\":[21000,19000,21000,19000],\n",
    "        \"premium\":[72,199,74,201]\n",
    "    }\n",
    ")\n",
    "option_matrix_result = option_matrix(ticker='^HSI',option_holding=option_holding).astype(int)\n",
    "option_matrix_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
